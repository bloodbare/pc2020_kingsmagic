{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.7.7 64-bit ('ml')",
   "metadata": {
    "interpreter": {
     "hash": "565b996fdae56f9bd599fc6f17971bff5fccd3e3a3f7e8c46c6e77350e036155"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "                 id                                       comment_text  toxic  \\\n",
       "0  0000997932d777bf  Explanation\\nWhy the edits made under my usern...    0.0   \n",
       "1  000103f0d9cfb60f  D'aww! He matches this background colour I'm s...    0.0   \n",
       "2  000113f07ec002fd  Hey man, I'm really not trying to edit war. It...    0.0   \n",
       "3  0001b41b1c6bb37e  \"\\nMore\\nI can't make any real suggestions on ...    0.0   \n",
       "4  0001d958c54c6e35  You, sir, are my hero. Any chance you remember...    0.0   \n",
       "\n",
       "   severe_toxic  obscene  threat  insult  identity_hate  \n",
       "0           0.0      0.0     0.0     0.0            0.0  \n",
       "1           0.0      0.0     0.0     0.0            0.0  \n",
       "2           0.0      0.0     0.0     0.0            0.0  \n",
       "3           0.0      0.0     0.0     0.0            0.0  \n",
       "4           0.0      0.0     0.0     0.0            0.0  "
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>comment_text</th>\n      <th>toxic</th>\n      <th>severe_toxic</th>\n      <th>obscene</th>\n      <th>threat</th>\n      <th>insult</th>\n      <th>identity_hate</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0000997932d777bf</td>\n      <td>Explanation\\nWhy the edits made under my usern...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>000103f0d9cfb60f</td>\n      <td>D'aww! He matches this background colour I'm s...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>000113f07ec002fd</td>\n      <td>Hey man, I'm really not trying to edit war. It...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0001b41b1c6bb37e</td>\n      <td>\"\\nMore\\nI can't make any real suggestions on ...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0001d958c54c6e35</td>\n      <td>You, sir, are my hero. Any chance you remember...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 4
    }
   ],
   "source": [
    "from absl import logging\n",
    "\n",
    "import tensorflow.compat.v1 as tf\n",
    "tf.disable_v2_behavior()\n",
    "\n",
    "import tensorflow_hub as hub\n",
    "import sentencepiece as spm\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "train_data_path='./data/train copy.csv'\n",
    "train = pd.read_csv(train_data_path)\n",
    "test = pd.read_csv('./data/test.csv')\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n",
      "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n",
      "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n",
      "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n",
      "SentencePiece model loaded at b'./model/assets/universal_encoder_8k_spm.model'.\n"
     ]
    }
   ],
   "source": [
    "module = hub.Module(\"./model/\")\n",
    "\n",
    "input_placeholder = tf.sparse_placeholder(tf.int64, shape=[None, None])\n",
    "encodings = module(\n",
    "    inputs=dict(\n",
    "        values=input_placeholder.values,\n",
    "        indices=input_placeholder.indices,\n",
    "        dense_shape=input_placeholder.dense_shape))\n",
    "\n",
    "with tf.Session() as sess:\n",
    "  spm_path = sess.run(module(signature=\"spm_path\"))\n",
    "\n",
    "sp = spm.SentencePieceProcessor()\n",
    "sp.Load(spm_path)\n",
    "print(\"SentencePiece model loaded at {}.\".format(spm_path))\n",
    "\n",
    "def process_to_IDs_in_sparse_format(sp, sentences):\n",
    "  # An utility method that processes sentences with the sentence piece processor\n",
    "  # 'sp' and returns the results in tf.SparseTensor-similar format:\n",
    "  # (values, indices, dense_shape)\n",
    "  ids = [sp.EncodeAsIds(x) for x in sentences]\n",
    "  max_len = max(len(x) for x in ids)\n",
    "  dense_shape=(len(ids), max_len)\n",
    "  values=[item for sublist in ids for item in sublist]\n",
    "  indices=[[row,col] for row in range(len(ids)) for col in range(len(ids[row]))]\n",
    "  return (values, indices, dense_shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Doing step 0\n",
      " - going to session 0\n",
      " - saving 0\n",
      "Doing step 1\n",
      " - going to session 1\n",
      " - saving 1\n",
      "Doing step 2\n",
      " - going to session 2\n",
      " - saving 2\n",
      "Doing step 3\n",
      " - going to session 3\n",
      " - saving 3\n",
      "Doing step 4\n",
      " - going to session 4\n",
      " - saving 4\n",
      "Doing step 5\n",
      " - going to session 5\n",
      " - saving 5\n",
      "Doing step 6\n",
      " - going to session 6\n",
      " - saving 6\n",
      "Doing step 7\n",
      " - going to session 7\n",
      " - saving 7\n",
      "Doing step 8\n",
      " - going to session 8\n",
      " - saving 8\n",
      "Doing step 9\n",
      " - going to session 9\n",
      " - saving 9\n",
      "Doing step 10\n",
      " - going to session 10\n",
      " - saving 10\n",
      "Doing step 11\n",
      " - going to session 11\n",
      " - saving 11\n",
      "Doing step 12\n",
      " - going to session 12\n",
      " - saving 12\n",
      "Doing step 13\n",
      " - going to session 13\n",
      " - saving 13\n",
      "Doing step 14\n",
      " - going to session 14\n",
      " - saving 14\n"
     ]
    }
   ],
   "source": [
    "from cleantext import clean\n",
    "\n",
    "length = len(test['comment_text'])\n",
    "for i in range(length//10000):\n",
    "    print(f\"Doing step {i}\")\n",
    "    init = 10000 * i\n",
    "    end = 10000 * (i + 1)\n",
    "    if end > length:\n",
    "        end = length\n",
    "    subresult = []\n",
    "    for sentence in test['comment_text'][init:end]:\n",
    "        subresult.append(clean(sentence, no_punct=True, no_numbers=True, no_line_breaks=True, lower=True, to_ascii=True))\n",
    "    values, indices, dense_shape = process_to_IDs_in_sparse_format(sp, subresult)\n",
    "    # Reduce logging output.\n",
    "    logging.set_verbosity(logging.ERROR)\n",
    "\n",
    "    print(f\" - going to session {i}\")\n",
    "    with tf.Session() as session:\n",
    "        session.run([tf.global_variables_initializer(), tf.tables_initializer()])\n",
    "        message_embeddings = session.run(\n",
    "            encodings,\n",
    "            feed_dict={input_placeholder.values: values,\n",
    "                        input_placeholder.indices: indices,\n",
    "                        input_placeholder.dense_shape: dense_shape})\n",
    "\n",
    "    print(f\" - saving {i}\")\n",
    "    with open(f'vectors_test{i}', 'wb') as f:\n",
    "        np.save(f, np.array(message_embeddings).tolist())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "159583"
      ]
     },
     "metadata": {},
     "execution_count": 6
    }
   ],
   "source": [
    "len(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "WARNING:root:Since the GPL-licensed package `unidecode` is not installed, using Python's `unicodedata` package which yields worse results.\n",
      "Doing step 0\n",
      " - going to session 0\n",
      " - saving 0\n",
      "Doing step 1\n",
      " - going to session 1\n",
      " - saving 1\n",
      "Doing step 2\n",
      " - going to session 2\n",
      " - saving 2\n",
      "Doing step 3\n",
      " - going to session 3\n",
      " - saving 3\n",
      "Doing step 4\n",
      " - going to session 4\n",
      " - saving 4\n",
      "Doing step 5\n",
      " - going to session 5\n",
      " - saving 5\n",
      "Doing step 6\n",
      " - going to session 6\n",
      " - saving 6\n",
      "Doing step 7\n",
      " - going to session 7\n",
      " - saving 7\n",
      "Doing step 8\n",
      " - going to session 8\n",
      " - saving 8\n",
      "Doing step 9\n",
      " - going to session 9\n",
      " - saving 9\n",
      "Doing step 10\n",
      " - going to session 10\n",
      " - saving 10\n",
      "Doing step 11\n",
      " - going to session 11\n",
      " - saving 11\n",
      "Doing step 12\n",
      " - going to session 12\n",
      " - saving 12\n",
      "Doing step 13\n",
      " - going to session 13\n",
      " - saving 13\n",
      "Doing step 14\n",
      " - going to session 14\n",
      " - saving 14\n",
      "Doing step 15\n",
      " - going to session 15\n",
      " - saving 15\n"
     ]
    }
   ],
   "source": [
    "from cleantext import clean\n",
    "\n",
    "length = len(train['comment_text'])\n",
    "for i in range((length//10000 + 1)):\n",
    "    print(f\"Doing step {i}\")\n",
    "    init = 10000 * i\n",
    "    end = 10000 * (i + 1)\n",
    "    if end > length:\n",
    "        end = length\n",
    "    subresult = []\n",
    "    for sentence in train['comment_text'][init:end]:\n",
    "        subresult.append(clean(sentence, no_punct=True, no_numbers=True, no_line_breaks=True, lower=True, to_ascii=True))\n",
    "    values, indices, dense_shape = process_to_IDs_in_sparse_format(sp, subresult)\n",
    "    # Reduce logging output.\n",
    "    logging.set_verbosity(logging.ERROR)\n",
    "\n",
    "    print(f\" - going to session {i}\")\n",
    "    with tf.Session() as session:\n",
    "        session.run([tf.global_variables_initializer(), tf.tables_initializer()])\n",
    "        message_embeddings = session.run(\n",
    "            encodings,\n",
    "            feed_dict={input_placeholder.values: values,\n",
    "                        input_placeholder.indices: indices,\n",
    "                        input_placeholder.dense_shape: dense_shape})\n",
    "\n",
    "    print(f\" - saving {i}\")\n",
    "    with open(f'vectors{i}', 'wb') as f:\n",
    "        np.save(f, np.array(message_embeddings).tolist())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}